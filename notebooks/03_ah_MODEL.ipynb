{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12934a2d-acbd-4596-a9a5-5e8691b6b07a",
   "metadata": {},
   "source": [
    "# Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cae1ecb-ba50-4e9d-808a-8276bacdb12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "from src.config import (\n",
    "    CLEAN_DATA,\n",
    "    MODELS_FOLDER,\n",
    "    KAGGLE_SUBMISSION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b28c57-4538-4bbd-8c11-ab6e1f54e242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 82)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CLEAN_DATA)\n",
    "\n",
    "print(df.shape)\n",
    "df_train = df.loc[~df['SalePrice'].isnull()]\n",
    "df_test = df.loc[df['SalePrice'].isnull()]\n",
    "\n",
    "id = df_test['Id']\n",
    "\n",
    "df_train = df_train.drop('Id', axis=1)\n",
    "df_test = df_test.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeee85b8-3f83-42fc-a160-35acf0a55272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Wed Dec 11 22:24:04 UTC 2024\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.81 GB / 7.56 GB (50.4%)\n",
      "Disk Space Avail:   1557.68 GB / 1832.70 GB (85.0%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800.0s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-01-28 10:52:24,333\tINFO worker.py:1762 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Beginning AutoGluon training ... Time limit = 433s\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m AutoGluon will save models to \"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Train Data Rows:    1297\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Train Data Columns: 80\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Label Column:       SalePrice\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tAvailable Memory:                    2670.95 MB\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTrain Data (Original)  Memory Usage: 3.57 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('float', [])  : 12 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('int', [])    : 25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('float', [])     : 12 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('int', [])       : 25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t\t('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t80 features in original data used to generate 80 features in processed data.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.44 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'NN_TORCH': [{}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'XGB': [{}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'FASTAI': [{}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 288.12s of the 432.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.67%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=25558)\u001b[0m [1000]\tvalid_set's rmse: 0.126927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1239\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t3.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 279.75s of the 423.92s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.75%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=25886)\u001b[0m [1000]\tvalid_set's rmse: 0.133901\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1283\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t3.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 273.84s of the 418.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1405\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t2.84s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 270.88s of the 415.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.33%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1245\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t87.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 181.42s of the 325.59s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1395\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 180.37s of the 324.54s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.18%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1296\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t16.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 157.44s of the 301.61s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=26675)\u001b[0m No improvement since epoch 8: early stopping\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.44%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.133\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t7.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 134.87s of the 279.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1256\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t29.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.46s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 103.67s of the 247.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.13%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.144\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t8.55s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 236.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.304, 'LightGBMXT_BAG_L1': 0.217, 'LightGBM_BAG_L1': 0.174, 'NeuralNetFastAI_BAG_L1': 0.174, 'CatBoost_BAG_L1': 0.087, 'XGBoost_BAG_L1': 0.043}\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1165\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 9 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 236.65s of the 236.64s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1246\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t3.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 230.98s of the 230.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.68%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1241\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t3.42s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 225.15s of the 225.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1204\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t2.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 222.48s of the 222.47s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.24%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1215\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t45.85s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 174.06s of the 174.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1196\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t1.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 172.89s of the 172.88s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.18%)\n",
      "\u001b[36m(_ray_fit pid=29209)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1287\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t10.07s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.45s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 160.68s of the 160.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.20%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1283\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t6.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 151.70s of the 151.69s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=29208)\u001b[0m No improvement since epoch 9: early stopping\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1257\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t16.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 133.00s of the 132.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.19%)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1292\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t7.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 123.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.24, 'RandomForestMSE_BAG_L2': 0.24, 'NeuralNetTorch_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.12, 'XGBoost_BAG_L1': 0.08, 'CatBoost_BAG_L1': 0.04, 'ExtraTreesMSE_BAG_L2': 0.04, 'NeuralNetFastAI_BAG_L2': 0.04}\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t-0.1163\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m AutoGluon training complete, total runtime = 309.28s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 82.7 rows/s (163 batch size)\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t1.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t2.84s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t11.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStopping at the best epoch learned earlier - 18.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t22.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t22.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t1.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.304, 'LightGBMXT_BAG_L1': 0.217, 'LightGBM_BAG_L1': 0.174, 'NeuralNetFastAI_BAG_L1': 0.174, 'CatBoost_BAG_L1': 0.087, 'XGBoost_BAG_L1': 0.043}\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.37s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t2.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t5.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t1.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tStopping at the best epoch learned earlier - 10.\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t18.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t12.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.24, 'RandomForestMSE_BAG_L2': 0.24, 'NeuralNetTorch_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.12, 'XGBoost_BAG_L1': 0.08, 'CatBoost_BAG_L1': 0.04, 'ExtraTreesMSE_BAG_L2': 0.04, 'NeuralNetFastAI_BAG_L2': 0.04}\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m \t0.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Refit complete, total runtime = 100.75s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=25198)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     ExtraTreesMSE_BAG_L2_FULL      -0.100184  -0.119555  root_mean_squared_error        0.544883            NaN  63.220556                 0.070198                0.097389           1.032046            2       True         15\n",
      "1   RandomForestMSE_BAG_L2_FULL      -0.101472  -0.120393  root_mean_squared_error        0.568187            NaN  64.700505                 0.093502                0.111713           2.511995            2       True         13\n",
      "2      WeightedEnsemble_L3_FULL      -0.104865  -0.116268  root_mean_squared_error        0.704650            NaN  84.021503                 0.005019                     NaN           0.038735            3       True         20\n",
      "3          CatBoost_BAG_L1_FULL      -0.106658  -0.124541  root_mean_squared_error        0.060478            NaN  11.008083                 0.060478                     NaN          11.008083            1       True          4\n",
      "4      WeightedEnsemble_L2_FULL      -0.107221  -0.116463  root_mean_squared_error        0.326405            NaN  58.544223                 0.003533                     NaN           0.022050            2       True         10\n",
      "5          CatBoost_BAG_L2_FULL      -0.108055  -0.121498  root_mean_squared_error        0.491950            NaN  67.760602                 0.017266                     NaN           5.572092            2       True         14\n",
      "6     LightGBMLarge_BAG_L2_FULL      -0.108187  -0.129162  root_mean_squared_error        0.519963            NaN  63.087376                 0.045279                     NaN           0.898866            2       True         19\n",
      "7   NeuralNetFastAI_BAG_L2_FULL      -0.111023  -0.128666  root_mean_squared_error        0.535931            NaN  80.438727                 0.061246                     NaN          18.250217            2       True         16\n",
      "8          LightGBM_BAG_L2_FULL      -0.113176  -0.124052  root_mean_squared_error        0.522819            NaN  62.557572                 0.048135                     NaN           0.369062            2       True         12\n",
      "9   NeuralNetFastAI_BAG_L1_FULL      -0.115509  -0.129619  root_mean_squared_error        0.046318            NaN  22.898672                 0.046318                     NaN          22.898672            1       True          6\n",
      "10         LightGBM_BAG_L1_FULL      -0.115676  -0.128307  root_mean_squared_error        0.018162            NaN   0.364759                 0.018162                     NaN           0.364759            1       True          2\n",
      "11          XGBoost_BAG_L2_FULL      -0.115947  -0.128273  root_mean_squared_error        0.537128            NaN  62.564954                 0.062443                     NaN           0.376444            2       True         17\n",
      "12       LightGBMXT_BAG_L1_FULL      -0.117833  -0.123861  root_mean_squared_error        0.022995            NaN   1.046849                 0.022995                     NaN           1.046849            1       True          1\n",
      "13          XGBoost_BAG_L1_FULL      -0.117928  -0.133019  root_mean_squared_error        0.094939            NaN   0.630892                 0.094939                     NaN           0.630892            1       True          7\n",
      "14   NeuralNetTorch_BAG_L2_FULL      -0.118526  -0.125707  root_mean_squared_error        0.547929            NaN  74.506174                 0.073245                     NaN          12.317664            2       True         18\n",
      "15       LightGBMXT_BAG_L2_FULL      -0.120071  -0.124615  root_mean_squared_error        0.527556            NaN  62.451463                 0.052871                     NaN           0.262953            2       True         11\n",
      "16   NeuralNetTorch_BAG_L1_FULL      -0.120183  -0.125644  root_mean_squared_error        0.079979            NaN  22.572918                 0.079979                     NaN          22.572918            1       True          8\n",
      "17  RandomForestMSE_BAG_L1_FULL      -0.124184  -0.140501  root_mean_squared_error        0.078347       0.093890   2.837353                 0.078347                0.093890           2.837353            1       True          3\n",
      "18    LightGBMLarge_BAG_L1_FULL      -0.124426  -0.143982  root_mean_squared_error        0.051962            NaN   1.696691                 0.051962                     NaN           1.696691            1       True          9\n",
      "19    ExtraTreesMSE_BAG_L1_FULL      -0.129793  -0.139482  root_mean_squared_error        0.073465       0.153654   0.828985                 0.073465                0.153654           0.828985            1       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t431s\t = DyStack   runtime |\t1369s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1369s\n",
      "AutoGluon will save models to \"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models\"\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 80\n",
      "Label Column:       SalePrice\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2576.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.02 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 12 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\t\t('int', [])    : 25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n",
      "\t\t('float', [])     : 12 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\t\t('int', [])       : 25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n",
      "\t0.5s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 912.28s of the 1368.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.62%)\n",
      "\t-0.1214\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.66s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 900.88s of the 1357.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.76%)\n",
      "\t-0.1289\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.66s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 885.05s of the 1341.52s of remaining time.\n",
      "\t-0.1385\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 882.34s of the 1338.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.30%)\n",
      "\t-0.1214\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.41s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 722.74s of the 1179.22s of remaining time.\n",
      "\t-0.1363\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 721.73s of the 1178.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.20%)\n",
      "\t-0.1286\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.77s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 691.21s of the 1147.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.21%)\n",
      "\t-0.132\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.68s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 679.45s of the 1135.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.10%)\n",
      "\t-0.1247\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.55s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 646.64s of the 1103.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.23%)\n",
      "\t-0.1368\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.02s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1086.52s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.267, 'NeuralNetTorch_BAG_L1': 0.267, 'CatBoost_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.133, 'LightGBM_BAG_L1': 0.067, 'XGBoost_BAG_L1': 0.067}\n",
      "\t-0.1155\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1086.49s of the 1086.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.68%)\n",
      "\t-0.1246\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1080.62s of the 1080.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.68%)\n",
      "\t-0.1208\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.43s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1070.29s of the 1070.29s of remaining time.\n",
      "\t-0.1183\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1067.59s of the 1067.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.21%)\n",
      "\t-0.1187\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.2s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1020.94s of the 1020.94s of remaining time.\n",
      "\t-0.1167\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1019.63s of the 1019.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.20%)\n",
      "\t-0.131\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.08s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1007.39s of the 1007.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.31%)\n",
      "\t-0.122\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.43s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 997.67s of the 997.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.11%)\n",
      "\t-0.1228\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.38s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 978.06s of the 978.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.93%)\n",
      "\t-0.1251\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.47s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 966.76s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.2, 'ExtraTreesMSE_BAG_L2': 0.2, 'NeuralNetTorch_BAG_L1': 0.15, 'RandomForestMSE_BAG_L2': 0.15, 'CatBoost_BAG_L1': 0.1, 'XGBoost_BAG_L2': 0.1, 'NeuralNetFastAI_BAG_L1': 0.05, 'NeuralNetTorch_BAG_L2': 0.05}\n",
      "\t-0.1146\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 402.65s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 75.7 rows/s (183 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t1.23s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t21.39s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 18.\n",
      "\t16.08s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t16.45s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t1.33s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t0.59s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\t0.21s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\t9.46s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.2, 'ExtraTreesMSE_BAG_L2': 0.2, 'NeuralNetTorch_BAG_L1': 0.15, 'RandomForestMSE_BAG_L2': 0.15, 'CatBoost_BAG_L1': 0.1, 'XGBoost_BAG_L2': 0.1, 'NeuralNetFastAI_BAG_L1': 0.05, 'NeuralNetTorch_BAG_L2': 0.05}\n",
      "\t0.04s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 69.67s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "Deleting model LightGBMXT_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestMSE_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/RandomForestMSE_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesMSE_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/ExtraTreesMSE_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestMSE_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/RandomForestMSE_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesMSE_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/ExtraTreesMSE_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under /home/alexhubbe/MEGA/data_science/portfolio/house_prices/models/models/WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run the fit method: 903.47 seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: log-transform the target variable\n",
    "df_train['SalePrice'] = np.log1p(df_train['SalePrice'])  # Log transform target\n",
    "\n",
    "# Identify the target variable\n",
    "label = 'SalePrice'\n",
    "\n",
    "eval_metric = 'rmse'\n",
    "\n",
    "presets = 'good_quality'\n",
    "#'medium_quality'\n",
    "#'good_quality'\n",
    "#'high_quality'\n",
    "#'best_quality'\n",
    "\n",
    "time_limit = 60 * 60 * 0.5\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor \n",
    "start_time = time.time()\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label, \n",
    "    eval_metric=eval_metric,\n",
    "    path=MODELS_FOLDER,\n",
    "    problem_type='regression'\n",
    ").fit(\n",
    "    train_data=df_train, \n",
    "    time_limit=time_limit, \n",
    "    presets=presets,\n",
    "    keep_only_best=True,\n",
    "    save_space=True\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to run the fit method: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f78be5-46c0-4522-b7e3-bbdab69b4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To load, use: \n",
    "# predictor = TabularPredictor.load(\"/home/alexhubbe/MEGA/data_science/portfolio/house_prices/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bec9264c-6fb8-482d-bba5-b52f2b87b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L3_FULL'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a9be2d-95d0-4764-90db-a51935693e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = predictor.predict(df_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "predictions = np.expm1(predictions)\n",
    "\n",
    "# Ensure predictions are non-negative (important after inverse transform)\n",
    "predictions = np.maximum(0, predictions)\n",
    "\n",
    "predictions\n",
    "# Create a submission file (Kaggle format)\n",
    "submission = pd.DataFrame({'Id': id, 'SalePrice': predictions})\n",
    "\n",
    "submission.to_csv(KAGGLE_SUBMISSION, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab587471-65ed-481d-9cc2-dc9fff50e062",
   "metadata": {},
   "source": [
    "P.S.: AutoGluon suggests that feature performance should be evaluated on the test dataset, but this is not feasible in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19020124-de4c-4a2f-9393-ca18a2f2e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 80 features using 1460 rows with 10 shuffle sets... Time limit: 1800.0s...\n",
      "\t1525.17s\t= Expected runtime (152.52s per shuffle set)\n",
      "\t665.9s\t= Actual runtime (Completed 10 of 10 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "feature_importance = predictor.feature_importance(df_train, time_limit = time_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b2cff5-b238-464c-b505-1694c379fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            importance    stddev       p_value   n  p99_high  \\\n",
      "GrLivArea                     0.066878  0.002153  2.979412e-15  10  0.069091   \n",
      "OverallQual                   0.054311  0.001678  2.059869e-15  10  0.056036   \n",
      "Median_n_Closest_SalePrice    0.027059  0.000941  5.964460e-15  10  0.028026   \n",
      "TotalBsmtSF                   0.021760  0.000952  4.702880e-14  10  0.022739   \n",
      "1stFlrSF                      0.021413  0.000895  3.128684e-14  10  0.022334   \n",
      "OverallCond                   0.020579  0.000888  4.167715e-14  10  0.021492   \n",
      "BsmtFinSF1                    0.015972  0.000493  2.044923e-15  10  0.016478   \n",
      "Neighborhood                  0.012654  0.000394  2.220548e-15  10  0.013060   \n",
      "2ndFlrSF                      0.011216  0.000632  4.551984e-13  10  0.011866   \n",
      "LotArea                       0.011131  0.000704  1.285180e-12  10  0.011855   \n",
      "YearBuilt                     0.010849  0.000386  7.329332e-15  10  0.011245   \n",
      "YearRemodAdd                  0.009395  0.000521  3.918016e-13  10  0.009930   \n",
      "GarageCars                    0.009201  0.000446  1.188887e-13  10  0.009660   \n",
      "SaleCondition                 0.006910  0.000486  3.321866e-12  10  0.007410   \n",
      "GarageArea                    0.006819  0.000338  1.428041e-13  10  0.007166   \n",
      "Fireplaces                    0.006723  0.000596  2.635091e-11  10  0.007335   \n",
      "GarageYrBlt                   0.005291  0.000202  1.377552e-14  10  0.005499   \n",
      "BsmtFullBath                  0.004766  0.000256  2.923943e-13  10  0.005029   \n",
      "OpenPorchSF                   0.004489  0.000203  6.404787e-14  10  0.004698   \n",
      "WoodDeckSF                    0.004482  0.000277  1.033191e-12  10  0.004767   \n",
      "FullBath                      0.004433  0.000265  7.781216e-13  10  0.004706   \n",
      "HalfBath                      0.004383  0.000380  2.145313e-11  10  0.004773   \n",
      "LotFrontage                   0.004264  0.000314  5.050798e-12  10  0.004587   \n",
      "BsmtExposure                  0.003781  0.000194  1.990081e-13  10  0.003980   \n",
      "KitchenQual                   0.003588  0.000313  2.274340e-11  10  0.003909   \n",
      "CentralAir                    0.003445  0.000222  1.507888e-12  10  0.003673   \n",
      "BsmtQual                      0.003406  0.000179  2.425904e-13  10  0.003590   \n",
      "BsmtUnfSF                     0.003397  0.000236  2.994867e-12  10  0.003639   \n",
      "MSZoning                      0.003355  0.000243  4.272520e-12  10  0.003604   \n",
      "Condition1                    0.003333  0.000248  5.587641e-12  10  0.003588   \n",
      "Functional                    0.003307  0.000307  4.033139e-11  10  0.003623   \n",
      "Exterior1st                   0.003090  0.000197  1.365714e-12  10  0.003293   \n",
      "BsmtFinType1                  0.003071  0.000325  1.289726e-10  10  0.003405   \n",
      "ExterQual                     0.003022  0.000133  4.879762e-14  10  0.003159   \n",
      "YrSold                        0.002819  0.000189  2.214952e-12  10  0.003014   \n",
      "MasVnrArea                    0.002707  0.000187  2.825719e-12  10  0.002900   \n",
      "HeatingQC                     0.002706  0.000281  1.077566e-10  10  0.002994   \n",
      "ScreenPorch                   0.002698  0.000366  1.168395e-09  10  0.003074   \n",
      "TotRmsAbvGrd                  0.002647  0.000202  6.954924e-12  10  0.002855   \n",
      "MoSold                        0.002633  0.000230  2.326718e-11  10  0.002869   \n",
      "Exterior2nd                   0.002383  0.000080  4.370628e-15  10  0.002466   \n",
      "SaleType                      0.002366  0.000139  6.726953e-13  10  0.002509   \n",
      "MSSubClass                    0.002282  0.000167  4.851284e-12  10  0.002454   \n",
      "GarageFinish                  0.002061  0.000085  2.872470e-14  10  0.002149   \n",
      "EnclosedPorch                 0.001967  0.000165  1.618229e-11  10  0.002137   \n",
      "BedroomAbvGr                  0.001791  0.000120  2.221058e-12  10  0.001915   \n",
      "MasVnrType                    0.001755  0.000130  5.440806e-12  10  0.001889   \n",
      "FireplaceQu                   0.001730  0.000239  1.372137e-09  10  0.001975   \n",
      "Foundation                    0.001577  0.000137  2.279684e-11  10  0.001718   \n",
      "LotConfig                     0.001569  0.000186  3.504766e-10  10  0.001760   \n",
      "LotShape                      0.001555  0.000133  1.984791e-11  10  0.001692   \n",
      "BsmtFinSF2                    0.001294  0.000224  9.986718e-09  10  0.001524   \n",
      "HouseStyle                    0.001197  0.000113  4.511774e-11  10  0.001313   \n",
      "GarageType                    0.001164  0.000116  7.527953e-11  10  0.001284   \n",
      "ExterCond                     0.001055  0.000152  1.950853e-09  10  0.001210   \n",
      "KitchenAbvGr                  0.000993  0.000213  6.649980e-08  10  0.001213   \n",
      "LandContour                   0.000964  0.000108  2.210921e-10  10  0.001075   \n",
      "BsmtHalfBath                  0.000850  0.000198  1.347162e-07  10  0.001053   \n",
      "Fence                         0.000832  0.000084  8.727106e-11  10  0.000919   \n",
      "PavedDrive                    0.000771  0.000161  5.327176e-08  10  0.000937   \n",
      "GarageQual                    0.000638  0.000091  1.802478e-09  10  0.000731   \n",
      "BldgType                      0.000537  0.000087  5.882828e-09  10  0.000627   \n",
      "BsmtFinType2                  0.000535  0.000043  1.190810e-11  10  0.000580   \n",
      "GarageCond                    0.000533  0.000073  1.312666e-09  10  0.000608   \n",
      "RoofStyle                     0.000479  0.000089  1.831735e-08  10  0.000571   \n",
      "PoolArea                      0.000474  0.000191  1.260330e-05  10  0.000670   \n",
      "Alley                         0.000457  0.000086  1.983149e-08  10  0.000545   \n",
      "MiscVal                       0.000436  0.000163  7.036994e-06  10  0.000603   \n",
      "BsmtCond                      0.000427  0.000077  1.376174e-08  10  0.000506   \n",
      "3SsnPorch                     0.000408  0.000107  3.713922e-07  10  0.000518   \n",
      "Electrical                    0.000375  0.000049  7.704695e-10  10  0.000425   \n",
      "LandSlope                     0.000374  0.000048  7.375637e-10  10  0.000424   \n",
      "LowQualFinSF                  0.000363  0.000090  2.310777e-07  10  0.000455   \n",
      "Condition2                    0.000262  0.000044  8.008967e-09  10  0.000307   \n",
      "PoolQC                        0.000131  0.000010  1.036821e-11  10  0.000142   \n",
      "Utilities                     0.000109  0.000138  1.680855e-02  10  0.000251   \n",
      "RoofMatl                      0.000101  0.000029  7.351183e-07  10  0.000131   \n",
      "Street                        0.000093  0.000115  1.523083e-02  10  0.000212   \n",
      "Heating                       0.000061  0.000049  1.661369e-03  10  0.000112   \n",
      "MiscFeature                   0.000053  0.000020  6.803983e-06  10  0.000074   \n",
      "\n",
      "                             p99_low  \n",
      "GrLivArea                   0.064666  \n",
      "OverallQual                 0.052587  \n",
      "Median_n_Closest_SalePrice  0.026092  \n",
      "TotalBsmtSF                 0.020782  \n",
      "1stFlrSF                    0.020493  \n",
      "OverallCond                 0.019665  \n",
      "BsmtFinSF1                  0.015465  \n",
      "Neighborhood                0.012249  \n",
      "2ndFlrSF                    0.010567  \n",
      "LotArea                     0.010408  \n",
      "YearBuilt                   0.010452  \n",
      "YearRemodAdd                0.008860  \n",
      "GarageCars                  0.008742  \n",
      "SaleCondition               0.006411  \n",
      "GarageArea                  0.006472  \n",
      "Fireplaces                  0.006111  \n",
      "GarageYrBlt                 0.005084  \n",
      "BsmtFullBath                0.004503  \n",
      "OpenPorchSF                 0.004280  \n",
      "WoodDeckSF                  0.004198  \n",
      "FullBath                    0.004161  \n",
      "HalfBath                    0.003993  \n",
      "LotFrontage                 0.003942  \n",
      "BsmtExposure                0.003581  \n",
      "KitchenQual                 0.003266  \n",
      "CentralAir                  0.003217  \n",
      "BsmtQual                    0.003222  \n",
      "BsmtUnfSF                   0.003154  \n",
      "MSZoning                    0.003106  \n",
      "Condition1                  0.003078  \n",
      "Functional                  0.002991  \n",
      "Exterior1st                 0.002888  \n",
      "BsmtFinType1                0.002737  \n",
      "ExterQual                   0.002886  \n",
      "YrSold                      0.002624  \n",
      "MasVnrArea                  0.002515  \n",
      "HeatingQC                   0.002417  \n",
      "ScreenPorch                 0.002322  \n",
      "TotRmsAbvGrd                0.002440  \n",
      "MoSold                      0.002396  \n",
      "Exterior2nd                 0.002301  \n",
      "SaleType                    0.002223  \n",
      "MSSubClass                  0.002110  \n",
      "GarageFinish                0.001973  \n",
      "EnclosedPorch               0.001797  \n",
      "BedroomAbvGr                0.001667  \n",
      "MasVnrType                  0.001621  \n",
      "FireplaceQu                 0.001484  \n",
      "Foundation                  0.001436  \n",
      "LotConfig                   0.001378  \n",
      "LotShape                    0.001417  \n",
      "BsmtFinSF2                  0.001064  \n",
      "HouseStyle                  0.001081  \n",
      "GarageType                  0.001045  \n",
      "ExterCond                   0.000899  \n",
      "KitchenAbvGr                0.000774  \n",
      "LandContour                 0.000852  \n",
      "BsmtHalfBath                0.000646  \n",
      "Fence                       0.000746  \n",
      "PavedDrive                  0.000605  \n",
      "GarageQual                  0.000544  \n",
      "BldgType                    0.000447  \n",
      "BsmtFinType2                0.000491  \n",
      "GarageCond                  0.000458  \n",
      "RoofStyle                   0.000388  \n",
      "PoolArea                    0.000278  \n",
      "Alley                       0.000370  \n",
      "MiscVal                     0.000268  \n",
      "BsmtCond                    0.000348  \n",
      "3SsnPorch                   0.000298  \n",
      "Electrical                  0.000325  \n",
      "LandSlope                   0.000325  \n",
      "LowQualFinSF                0.000270  \n",
      "Condition2                  0.000216  \n",
      "PoolQC                      0.000120  \n",
      "Utilities                  -0.000032  \n",
      "RoofMatl                    0.000072  \n",
      "Street                     -0.000025  \n",
      "Heating                     0.000011  \n",
      "MiscFeature                 0.000033  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(feature_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:autogluon]",
   "language": "python",
   "name": "conda-env-autogluon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
